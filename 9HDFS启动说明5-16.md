###1111



## use hadoop 
1.dui对外提供服务 ?和集群
2.第一次驱动我们是配置当前xxx的无密码信任



处理再次shh登陆后要输入密码

```
[hadoop@hadoop000 ~]$ rm -rf .ssh
[hadoop@hadoop000 ~]$ ssh-keygen
[hadoop@hadoop000 ~]$ cd .ssh
[hadoop@hadoop000 .ssh]$ ll
total 8
-rw-------. 1 hadoop hadoop 1675 May 16 21:44 id_rsa
-rw-r--r--. 1 hadoop hadoop  398 May 16 21:44 id_rsa.pub
[hadoop@hadoop000 .ssh]$ cat id_rsa.pub >> authorized_keys
[hadoop@hadoop000 .ssh]$ chmod 600 authorized_keys
```

开服务报错
/home/cs/hadoop-2.8.11/etc/hadoop/hadoop-env.sh: line 18: export: `=/home/cs/jdk1.8.0_161': not a valid identifier
Starting namenodes on [138.147.166.7


这个是hadoop-env的格式写错了






YARN伪分布式部署
[hadoop@hadoop000 hadoop]$ cp mapred-site.xml.template mapred-site.xml  记得备份
[hadoop@hadoop000 hadoop]$ vi mapred-site.xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>


[hadoop@hadoop000 hadoop]$ vi yarn-site.xml:
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    
    
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>ruozedata001:38088</value>
    </property>
</configuration>
记得修改yarn 8088端口 防止被攻击


[hadoop@hadoop000 sbin]$ ./start-yarn.sh



yarn 的两个进程
4308 SecondaryNameNode
5511 ResourceManager





#  tmp 默认30天内会清楚数据 

 `````
[root@aiffdata sbin]# hostnamectl
   Static hostname: localhost.localdomain
Transient hostname: aiffdata
         Icon name: computer-vm
           Chassis: vm
        Machine ID: 1ff6586e993048708cb7c8947531603c
           Boot ID: 8b8a89273a2e42be982e7dcf0b91556e
    Virtualization: vmware
  Operating System: CentOS Linux 7 (Core)
       CPE OS Name: cpe:/o:centos:centos:7
            Kernel: Linux 3.10.0-957.el7.x86_64
      Architecture: x86-64
      ```
      OOM：out of memory，指在linux里面，由于系统内存压力，系统会选择保护一些系统进程，而将一些其他的进程kill掉，释放内存。怎么避免进程因为OOM机制被kill掉？1.  与OOM相关的几个文件是 /proc//oom_adj 、 /proc//oom_score。前者是一个权值-16至15，默认是0，设置为-17表示永远不被kill，其余情况值越大越容易被kill。后者就是它计算出来的一个值，就是根据这个值来选择哪些进程被kill掉的。2.  上面放置使用swap中的第三个方法 overcommit参数，因为它分配不出内存就会返回错误，所以永远也不能达到内存被耗尽。OOM也就不会有影响了。

作者：徐文江
链接：https://www.zhihu.com/question/21972130/answer/19906482
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



 hadoop-2.8.1]$ hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar pi 5 10
  很吃内存  记得操作小心  

##  笔记  以后进程挂了--》log位置--》error: 有error具体分析；没 想到oom机制
free -m
cat /var/log/messages | grep oom
Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=590
        File Output Format Counters 
                Bytes Written=97
Job Finished in 262.832 seconds
Estimated value of Pi is 3.28000000000000000000  大概结果 数据不够大


web地址  http://192.168.174.133:8088/cluster  牛逼不牛逼  鲜花刷起来！
![pi例子](https://github.com/aiff/bigdata/blob/master/img/pi.png)
        


